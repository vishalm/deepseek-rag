# **Setting Up Ollama & Running DeepSeek R1 Locally for a Powerful RAG System**  

## **ğŸ”¹ AI | RAG | Python | DeepSeek**  

## **ğŸ¤– Ollama**  
Ollama is a framework for running large language models (LLMs) locally on your machine. It allows you to download, run, and interact with AI models without relying on cloud-based APIs.  

### **Why use it?**  
âœ… Free  
âœ… Private & Secure  
âœ… Fast  
âœ… Works Offline  

### **Example Usage:**  
```sh
ollama run deepseek-r1:1.5b
```
This command runs DeepSeek R1 locally.  

---  

## **ğŸ”— LangChain**  
LangChain is a Python/JavaScript framework that helps integrate LLMs with external data sources, APIs, and memory.  

### **Why use it?**  
- Connects LLMs to real-world applications like chatbots, document processing, and Retrieval-Augmented Generation (RAG).  

---  

## **ğŸ“„ What is RAG (Retrieval-Augmented Generation)?**  
RAG is an AI technique that retrieves external data (e.g., PDFs, databases) and augments the LLMâ€™s response.  

### **Why use it?**  
âœ… Improves accuracy  
âœ… Reduces hallucinations  
âœ… Enhances LLM responses with real-world data  

### **Example:**  
AI-powered PDF Q&A system that retrieves relevant document content before generating answers.  

---  

## **âš¡ DeepSeek R1**  
DeepSeek R1 is an open-source AI model optimized for reasoning, problem-solving, and factual retrieval.  

### **Why use it?**  
âœ… Strong logical capabilities  
âœ… Great for RAG applications  
âœ… Can be run locally with Ollama  

---  

## **ğŸš€ How They Work Together?**  
1ï¸âƒ£ **Ollama** runs DeepSeek R1 locally.  
2ï¸âƒ£ **LangChain** connects the AI model to external data.  
3ï¸âƒ£ **RAG** enhances responses by retrieving relevant information.  
4ï¸âƒ£ **DeepSeek R1** generates high-quality answers.  

### **ğŸ’¡ Example Use Case**  
A Q&A system that allows users to upload a PDF and ask questions about it, powered by DeepSeek R1 + RAG + LangChain on Ollama! ğŸš€  

---  

## **ğŸ¯ Why Run DeepSeek R1 Locally?**  

| Benefit         | Cloud-Based Models | Local DeepSeek R1 |
|----------------|--------------------|-------------------|
| **Privacy**    | âŒ Data sent to external servers | âœ… 100% Local & Secure |
| **Speed**      | â³ API latency & network delays | âš¡ Instant inference |
| **Cost**       | ğŸ’° Pay per API request | ğŸ†“ Free after setup |
| **Customization** | âŒ Limited fine-tuning | âœ… Full model control |
| **Deployment** | ğŸŒ Cloud-dependent | ğŸ”¥ Works offline & on-premises |

---

## **ğŸ›  Step 1: Installing Ollama**  
### **ğŸ”¹ Download Ollama**  
Ollama is available for macOS, Linux, and Windows.  

1ï¸âƒ£ Go to the [official Ollama download page](https://ollama.com/download)  
2ï¸âƒ£ Select your operating system  
3ï¸âƒ£ Click on the **Download** button  
4ï¸âƒ£ Install it following the system-specific instructions  

---

## **ğŸ›  Step 2: Running DeepSeek R1 on Ollama**  

### **ğŸ”¹ Pull the DeepSeek R1 Model**  
To download and set up the DeepSeek R1 (1.5B parameter model), run:  
```sh
ollama pull deepseek-r1:1.5b
```

### **ğŸ”¹ Running DeepSeek R1**  
After the model is downloaded, start interacting with it using:  
```sh
ollama run deepseek-r1:1.5b
```

---

## **ğŸ›  Step 3: Setting Up a RAG System Using Streamlit**  

### **ğŸ”¹ Prerequisites**  
Before running the RAG system, install the required dependencies:  

```sh
pip install -U langchain langchain-community
pip install streamlit
pip install pdfplumber
pip install semantic-chunkers
pip install open-text-embeddings
pip install faiss
pip install ollama
pip install prompt-template
pip install langchain_experimental
pip install sentence-transformers
pip install faiss-cpu
```

---

## **ğŸ›  Step 4: Running the RAG System**  

### **ğŸ”¹ Clone or Create the Project**  
```sh
mkdir rag-system && cd rag-system
```
Create a Python script `app.py` and paste the following code:

```python
import streamlit as st
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA

# Streamlit UI
st.title("ğŸ“„ RAG System with DeepSeek R1 & Ollama")

uploaded_file = st.file_uploader("Upload your PDF file here", type="pdf")

if uploaded_file:
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getvalue())

    loader = PDFPlumberLoader("temp.pdf")
    docs = loader.load()

    text_splitter = SemanticChunker(HuggingFaceEmbeddings())
    documents = text_splitter.split_documents(docs)

    embedder = HuggingFaceEmbeddings()
    vector = FAISS.from_documents(documents, embedder)
    retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    llm = Ollama(model="deepseek-r1:1.5b")

    prompt = """
    Use the following context to answer the question.
    Context: {context}
    Question: {question}
    Answer:"""

    QA_PROMPT = PromptTemplate.from_template(prompt)

    llm_chain = LLMChain(llm=llm, prompt=QA_PROMPT)
    combine_documents_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="context")

    qa = RetrievalQA(combine_documents_chain=combine_documents_chain, retriever=retriever)

    user_input = st.text_input("Ask a question about your document:")

    if user_input:
        response = qa(user_input)["result"]
        st.write("**Response:**")
        st.write(response)
```

---

## **ğŸ›  Step 5: Running the App**  
Once the script is ready, start your Streamlit app:  

```sh
streamlit run app.py
```

---

## **ğŸš€ Summary**  
You now have a fully functional local RAG system powered by:  
âœ… **Ollama** for running DeepSeek R1 locally  
âœ… **LangChain** for integrating LLMs with data sources  
âœ… **Streamlit** for an interactive UI  

This setup enables **secure, fast, and cost-free** AI-powered document retrieval and Q&A. ğŸš€  
