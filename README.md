# **Setting Up Ollama & Running DeepSeek R1 Locally for a Powerful RAG System**  

## **ğŸ”¹ AI | RAG | Python | DeepSeek**  

## **ğŸ¤– Ollama**  
Ollama is a framework for running large language models (LLMs) locally on your machine. It allows you to download, run, and interact with AI models without relying on cloud-based APIs.  

#### streamlet UI
![alt text](resources/image-portal.png)

### **Why use it?**  
âœ… Free  
âœ… Private & Secure  
âœ… Fast  
âœ… Works Offline  

## Yes Docker Ready
`docker run -p 8501:8501 vismis/streamlit-app-deepseeker1-7b:1.0.0`

### **Example Usage:**  
```sh
ollama run deepseek-r1:1.5b
```
This command runs DeepSeek R1 locally.  

---  

## **ğŸ”— LangChain**  
LangChain is a Python/JavaScript framework that helps integrate LLMs with external data sources, APIs, and memory.  

### **Why use it?**  
- Connects LLMs to real-world applications like chatbots, document processing, and Retrieval-Augmented Generation (RAG).  

---  

## **ğŸ“„ What is RAG (Retrieval-Augmented Generation)?**  
RAG is an AI technique that retrieves external data (e.g., PDFs, databases) and augments the LLMâ€™s response.  

### **Why use it?**  
âœ… Improves accuracy  
âœ… Reduces hallucinations  
âœ… Enhances LLM responses with real-world data  

### **Example:**  
AI-powered PDF Q&A system that retrieves relevant document content before generating answers.  

---  

## **âš¡ DeepSeek R1**  
DeepSeek R1 is an open-source AI model optimized for reasoning, problem-solving, and factual retrieval.  

### **Why use it?**  
âœ… Strong logical capabilities  
âœ… Great for RAG applications  
âœ… Can be run locally with Ollama  

---  

## **ğŸš€ How They Work Together?**  
1ï¸âƒ£ **Ollama** runs DeepSeek R1 locally.  
2ï¸âƒ£ **LangChain** connects the AI model to external data.  
3ï¸âƒ£ **RAG** enhances responses by retrieving relevant information.  
4ï¸âƒ£ **DeepSeek R1** generates high-quality answers.  

### **ğŸ’¡ Example Use Case**  
A Q&A system that allows users to upload a PDF and ask questions about it, powered by DeepSeek R1 + RAG + LangChain on Ollama! ğŸš€  

---  

## **ğŸ¯ Why Run DeepSeek R1 Locally?**  

| Benefit         | Cloud-Based Models | Local DeepSeek R1 |
|----------------|--------------------|-------------------|
| **Privacy**    | âŒ Data sent to external servers | âœ… 100% Local & Secure |
| **Speed**      | â³ API latency & network delays | âš¡ Instant inference |
| **Cost**       | ğŸ’° Pay per API request | ğŸ†“ Free after setup |
| **Customization** | âŒ Limited fine-tuning | âœ… Full model control |
| **Deployment** | ğŸŒ Cloud-dependent | ğŸ”¥ Works offline & on-premises |

---

## **ğŸ›  Step 1: Installing Ollama**  
### **ğŸ”¹ Download Ollama**  
Ollama is available for macOS, Linux, and Windows.  

1ï¸âƒ£ Go to the [official Ollama download page](https://ollama.com/download)  
2ï¸âƒ£ Select your operating system  
3ï¸âƒ£ Click on the **Download** button  
4ï¸âƒ£ Install it following the system-specific instructions  

---

## **ğŸ›  Step 2: Running DeepSeek R1 on Ollama**  

### **ğŸ”¹ Pull the DeepSeek R1 Model**  
To download and set up the DeepSeek R1 (1.5B parameter model), run:  
```sh
ollama pull deepseek-r1:1.5b
```

### **ğŸ”¹ Running DeepSeek R1**  
After the model is downloaded, start interacting with it using:  
```sh
ollama run deepseek-r1:1.5b
```

---

## **ğŸ›  Step 3: Setting Up a RAG System Using Streamlit**  

### **ğŸ”¹ Prerequisites**  
Before running the RAG system, install the required dependencies:  

```sh
git clone https://github.com/vishalm/deepseek-rag.git
cd deepseek-rag
virtualenv venv
source venv
pip install -r requirement.txt
```

## **ğŸ›  Step 4: Running the App**  
Once the script is ready, start your Streamlit app:  

```sh
streamlit run app.py
```

---



### Model Information
```
tail -f  ~/.ollama/logs/server.log
```

##### Model Information
![alt text](/resources/image-1.png)

#### macmon

![alt text](/resources/image.png)


## **ğŸš€ Summary**  
You now have a fully functional local RAG system powered by:  
âœ… **Ollama** for running DeepSeek R1 locally  
âœ… **LangChain** for integrating LLMs with data sources  
âœ… **Streamlit** for an interactive UI  

This setup enables **secure, fast, and cost-free** AI-powered document retrieval and Q&A. ğŸš€  



1ï¸âƒ£ Build the Docker Image
Since we updated the Dockerfile to preload the model, run:

`docker build -t vismis/streamlit-app-deepseeker1-7b:1.0.0 .`
(Replace my-dockerhub-username with your actual Docker Hub username.)

2ï¸âƒ£ Verify the Image Locally
Run the container to make sure the model is already available:

`docker run -p 8501:8501 vismis/streamlit-app-deepseeker1-7b:1.0.0`
If everything works fine, proceed to the next step.

3ï¸âƒ£ Log in to Docker Hub
`docker login`
Enter your Docker Hub username and password when prompted.

4ï¸âƒ£ Tag the Image (If Not Done Before)
Docker requires a proper tag before pushing. If you built the image without specifying the full name, tag it now:
`docker tag vismis/streamlit-app-deepseeker1-7b:1.0.0 vismis/streamlit-app-deepseeker1-7b:1.0.0`

5ï¸âƒ£ Push the Image to Docker Hub
Now, push the image:

`docker push vismis/streamlit-app-deepseeker1-7b:1.0.0`

6ï¸âƒ£ Pull and Run on Any Machine
Once the image is pushed, you can pull and run it anywhere with:
`docker pull vismis/streamlit-app-deepseeker1-7b:1.0.0`
`docker run -p 8501:8501 vismis/streamlit-app-deepseeker1-7b:1.0.0`


#### *This will start your Streamlit app with the preloaded Ollama model. ğŸš€*